{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTJP3h3blcDm"
   },
   "source": [
    "# Recherche d'Information et traitement de données massives\n",
    "\n",
    "# Lab 4 : Recherche d’information sur le Web "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3yAdUK5lcDp"
   },
   "source": [
    "\n",
    "L'objectif de cette séance est d'étudier le problème de la recherche d'information sur le WEB. Elle constiendra deux parties.\n",
    " + La première partie s'intéresse aux approches permettant le passage à l'échelle des données du WEB et notamment au paradigme MapReduce. Vous pratiquerez notamment dans cette partie l'écriture d'algorithmes selon le cadre MapReduce.\n",
    " + La deuxième partie s'intéresse à la prise en compte de la structure de la collection du WEB au travers de la mise en oeuvre d'approches d'analyse de liens pour l'ordonnancement.\n",
    " + Plusieurs exercices d'approfondissement facultatifs vous sont aussi proposés.\n",
    "\n",
    "## PARTIE 1 :  RECHERCHE WEB et PASSAGE À l'ECHELLE\n",
    "## EXERCICES : écriture d'algorithmes en MapReduce (sur papier)\n",
    "\n",
    "### Avant propos : un bref rappel de MapReduce\n",
    "Comme nous l'avons vu dans le cours 4, Map Reduce est un **modèle de programmation** (ou patron de programmation) qui fournit un cadre pour automatiser le calcul distribué sur des données massives. Ce cadre propose d'écrire tout traitement à l'aide de deux opérations `map` et  `reduce` et de la représentation des données sous la forme de paires `(clé,valeur)`.\n",
    "MapReduce est aussi un **framework d'exécution** qui permet l'éxécution distribuée des programmes écrits selon ce cadre, et cela de manière totalement transparente selon le schéma rappelé ci-dessous.\n",
    "\n",
    "\n",
    "<img src=\"./Figures/map-reduce.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "1.  Un certain nombre de tâches *Map* sont alimentées par une ou\n",
    "    plusieurs partitions de données en provenance d'un système de\n",
    "    fichiers distribués (par exemple GFS, HDFS, S3).  Ces tâches *Map* transforment ces données en une séquence de paires clés-valeurs. C'est le développeur qui détermine comment sont calculées les paires\n",
    "    clés-valeurs en fonction des données en entrée en écrivant le code dans la fonction `map()`.\n",
    "2.  Les paires clés-valeurs sont collectées par un contrôleur maître et triées par clés.\n",
    "    Les paires sont redirigées vers les tâches *Reduce* de façon à ce que toutes les paires\n",
    "    qui ont la même clé soient redirigées vers la même tâche *Reduce*.\n",
    "3.  Les tâches *Reduce* traitent les clés une par une. Elles agrègent/combinent les valeurs associées\n",
    "    aux clés selon le code spécifié dans la fonction `reduce()`.\n",
    "\n",
    "\n",
    "\n",
    "Dans le cours, nous avons vu comment écrire en MapReduce un programme permettant de compter le nombre d'occurences des mots d'une collection donnée (programme **WordCount**). C'est une tâche qui peut sembler très simple à écrire quand on travaille sur une collection de petite taille. Vous avez d'ailleurs proposé une solution non-distribuée pour cette tâche pour la collection TIME (étape de filtrage par mots fréquents ou calcul de la pondération TF). En python, nous pouvons écrire cela très simplement, par exemple comme dans le programme ci-dessous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNjiI4lhlcDr"
   },
   "outputs": [],
   "source": [
    "def count_frequency(collection):\n",
    "    tokens_count={}\n",
    "    for doc_id in collection:\n",
    "        for token in collection[doc_id]:\n",
    "            if token in tokens_count.keys():\n",
    "                tokens_count[token] +=1\n",
    "            else:\n",
    "                tokens_count[token]=+1\n",
    "    return tokens_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCpHF-nmlcDz"
   },
   "source": [
    "Dans le cas où l'on considère une grosse collection dont le stokage et les traitements nécéssitent d'être distribués, alors cette tâche devient plus diffile à écrire, notamment car il faut prendre en compte la distribution des données et des traitements. Le modèle MapReduce apporte une solution à cela en proposant d'écrire ce type de programme selon le principe suivant :\n",
    "\n",
    "+ On considère que le document ou la partie du document est donné sous la forme d'une paire (clé,valeur) avec comme clé, l'identifiant du document et comme valeur le contenu textuel du document.\n",
    "\n",
    "+ Étant donnée une collection d’items, appliquer à chaque item un processus de transformation individuelle (étape `MAP`) qui produit des valeurs intermédiaires étiquetées. Dans le cas du WordCount, il s'agit juste de prendre chaque token du document ou de la partie du document et de la transformer en la paire (mot,1) comme illustré ci-dessous.\n",
    "\n",
    "<img src=\"./Figures/diapositive12.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "\n",
    "+ Regrouper ces valeurs intermédiaires par étiquette (étape faite par le framework `SHUFFLE AND SORT`). On aura dans le cas du WordCount en sortie de cette étape en ensemble de paires (mot, [1,1,1,1..]) avec comme clés les différents mots du documents et comme valeur une liste des 1-occurence des mots dans le document considéré.\n",
    "+ Appliquer une fonction d'agrégation à chaque groupe (étape `REDUCE`).Dans le cas du Wordcount il s'agit juste de sommer la liste des occurences comme illustré ci-dessous.\n",
    "\n",
    "<img src=\"./Figures/diapositive15.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "\n",
    "### Concevoir un programme selon le cadre MapReduce : un peu de méthodologie\n",
    "\n",
    "Pour faciliter l'écriture de programmes selon le cadre MapReduce, il est souvent nécessaire de se poser d'abord  les questions ci-dessous.\n",
    "\n",
    " + De quelle nature sont les documents en entrée ? Comment les représenter sous une forme `(clé, valeur)` ?\n",
    " + Quelles sont les groupes visés ? Quelles sont les valeurs intermédiaires que je cherche à produire depuis mon document d'entrée ?\n",
    " + Quelle est la valeur finale ? Quelle est la nature de l'agrégation pour produire cette valeur finale ? \n",
    "\n",
    "Une fois les réponses à ces questions claires, il suffira ensuite :\n",
    " + D'écrire la fonction de `map` qui prend en entrée un document et qui produit une séquence de paires `(clé, valeur)`.\n",
    " + D'avoir en tête que ces différentes paires sont collectées par le framework et triées par clés pour donner une entrée à la tâche d'agrégation l'ensemble des paires qui ont la même clé. \n",
    " + D'écrire la fonction `reduce` qui prend en entrée une paire `(clé, liste(valeurs))` en spécifiant le traitement d'agrégation voulu. \n",
    "\n",
    "Le schéma d'éxécution et sa trace sur le problème du WordCount vous est donné ci-dessous, pour rappel.\n",
    "\n",
    "<img src=\"./Figures/diapositive16.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "<img src=\"./Figures/diapositive17.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "Nous allons maintenant appliquer le cadre MapReduce au calcul de la pondération `TF-IDF`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D07OSBCklcD1"
   },
   "source": [
    "### A votre tour : Tf-IDF en MapReduce\n",
    "\n",
    "L'objectif est de calculer `Tf-IDF` pour un ensemble de documents en utilisant le modèle MapReduce. Pour rappel, comme vu dans le cours 1, `Tf-IDF (Term frequency-Inverse Document Frequency)` est une statistique qui traduit le niveau d'importance d'un terme $t$ pour un document $d$ appartenant à une collection (ou un corpus) de taille $N$. Dans cet exercice, on considèrera la formulation mathématique suivante :\n",
    "\n",
    "$$TF-IDF(t_i,d) = ( \\frac{tf_{t_i,d}}{\\sum_{t_k \\in d} tf_{t_k,d}}  ) \\times \\log \\left( \\frac{N}{df_t}\\right)$$\n",
    "\n",
    "où $tf_{t,d}$ est le nombre d'occurrence du terme $t$ dans le document $d$, $N$ le nombre de documents de la collection et $df_t$ le nombre de documents dans lesquels $t$ est présent.\n",
    "\n",
    "Vous pouvez prendre le temps de regarder la correction du Lab2 dans lequel nous avons proposé des solutions pour calculer cette statistique sur la collection TIME.\n",
    "\n",
    "Pour calculer cette statistique sur une très grosse collection de documents, il est nécéssaire de distribuer son calcul. Nous allons pour cela découper le travail en 3 étapes :\n",
    "\n",
    " + **1- Le calcul, pour chaque mot, de son nombre d'occurences par document.** Nous appelerons cette étape `WordFrequenceInDocs`.\n",
    " + **2- Le calcul du nombre de mots par documents**. Nous appelerons cette étape `WordCountsForDocs`.\n",
    " + **3- La combinaison des 2 informations précédentes pour calculer le TF_IDF**. Nous appelerons cette étape `WordsInCorpusTFIDF`.\n",
    " \n",
    "#### Etape 1 : WordFrequenceInDocs\n",
    "\n",
    "Il s'agit donc ici de formuler le problème du calcul du nombre d'occurences des mots d'un document avec le cadre MapReduce. C'est un problème très proche du problème du WordCount vu en cours et rappelé ci-dessus et vous devriez pouvoir y répondre rapidement.\n",
    "\n",
    "**Données d'entrée**\n",
    "\n",
    "Votre premier travail est de proposer une représentation adéquate de vos données d'entrée (une collection de document) pour le cadre MapReduce. On considère que chaque tâche `MAP` traitera un seul document (ou partie de document) et c'est donc ce document (ou cette partie) qui sera pris comme entrée de la fonction `MAP`.\n",
    "Que proposez-vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TwY_gfAlcD3"
   },
   "source": [
    "# A completer\n",
    "Un dictionnaire dont la clé est l'identifiant du document et le valeur est le document en soit (une string de texte par exemple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r63CLqfulcD8"
   },
   "source": [
    "**Fonction MAP**\n",
    "\n",
    "En vous inspirant du WordCount, écrire en pseudo-code, la fonction MAP pour cette étape. Attention, il faudra pouvoir garder l'information relative à l'identifiant du document considéré.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8-RW8iflcD9"
   },
   "outputs": [],
   "source": [
    "# A completer\n",
    "def my_map((doc_id, doc_content)):\n",
    "    tokens_count = []\n",
    "    for token in doc_content:\n",
    "        tokens_count.append((token, doc_id), 1)\n",
    "    return tokens_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gp3c-mGRlcEB"
   },
   "source": [
    "L'ensemble des paires (clé,valeur) provenant des différents noeuds `MAP` sont collectées et triées par clés intermédiaires et donc fournies sous cette forme à la tâche `REDUCE`. Ecrire, en pseudo-code la fonction `REDUCE` pour `WordFrequenceInDocs`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uYlPMXxwlcEE"
   },
   "outputs": [],
   "source": [
    "# A completer\n",
    "def my_reduce(tuple_word_doc_id, list_of_ones): #tuple_word_doc_id = (word, doc_id)\n",
    "    return (tuple_word_doc_id, len(list_of_ones)) #returns the term frequency of word in doc_id\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CY9qUu7elcEJ"
   },
   "source": [
    "#### Etape 2: WordCountsForDocs\n",
    "\n",
    "Il s'agit maintenant de calculer le nombre de mots par documents. Deux petites indications :\n",
    "+ L'entrée de cette tâche sera la sortie de la tâche précédente soit une paire de type \n",
    "`((word,doc_id),n)` avec `n` le nombre d'occurence du terme word dans le document `doc_id` soit le tf.\n",
    "+ Pour cette tâche, il pourrait être intéressant de pouvoir avoir accès aux documents par le mécanisme (clé,valeur).\n",
    "\n",
    "Ecrire, en pseudo-code, la fonction `MAP`, puis la fonction `REDUCE` vous permettant de faire cette étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMuX_wXjlcEK"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "#Map:\n",
    "def map_WordCountsForDocs(tuple,n): #tuple = (word,doc_id)\n",
    "    my_dict = {}\n",
    "    my_dict[doc_id] = n\n",
    "    return my_dict\n",
    "\n",
    "#Reduce:\n",
    "def reduce_WordCountsForDocs(doc_id, list_of_counts):\n",
    "    return (doc_id, sum(list_of_counts)) #return the number of words in doc_id\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwO0KHntlcEQ"
   },
   "source": [
    "#### Etape 3: WordsInCorpusTFIDF\n",
    "\n",
    "Il s'agit maintenant de combiner les informations précédentes pour calculer le TF-IDF pour chaque terme. On considère à nouveau ici que l'entrée est la sortie de la tâche précédente soit une paire de type \n",
    "`((word,doc_id),n/N)` avec `n` le nombre d'occurence du terme word dans le document `doc_id` et `N` le nombre total de mots dans le document `doc_id`. On supposera aussi que le nombre de documents $D$ dans la collection est passé au système sous la forme d'une constante.\n",
    "\n",
    "Ecrire, en pseudo-code, la fonction `MAP`, puis la fonction `REDUCE` vous permettant de faire cette étape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r3TNT6rklcES"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "def map_WordsInCorpusTFIDF( tuple, r): #tuple = (word,doc_id), r = n/N\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wOI-1LqlcEV"
   },
   "source": [
    "### A la maison :  Construction d'un index inversé en MapReduce\n",
    "\n",
    "Il s'agit ici de refléchir à comment un index inversé de documents peut être construit de manière distribuée à l'aide de MapReduce. \n",
    "\n",
    "1. Ecrivez en pseudo-code la fonction `MAP` en précisant bien le type des données d'entrée.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vgE8CAITlcEW"
   },
   "outputs": [],
   "source": [
    "# A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HXQ3WwrlcEa"
   },
   "source": [
    "2. Ecrivez en pseudo-code la fonction `REDUCE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XszSQNXXlcEc"
   },
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjucRujolcEf"
   },
   "source": [
    "3. Un test sur machine\n",
    "\n",
    "Pour vous permettre de mettre en oeuvre ce mécanisme de manière concrète, vous allez appliquer cela à l'indexation d'une collection [books.json](../Data/books.json).\n",
    "On cherche à produire un fichier inversé qui indique pour chaque mot, la liste des livres dans lesquels il apparaît en utilisant le cadre MapReduce.\n",
    "\n",
    "Pour cela, nous vous fournissons dans le répertoire [Utils](./Utils) un fichier [Lab4.py](./Utils/Lab4.py) qui contient un ensemble de fonctions qui vous seront utiles.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpIj2RZ7lcEg"
   },
   "source": [
    "Importer les fonctions utiles de ce module python à l'aide de la commande ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UOvVpRPlcEi"
   },
   "outputs": [],
   "source": [
    "from Utils.Lab4 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cunk9SnklcEl"
   },
   "source": [
    "A l'aide de la fonction `readData(filename)` de ce module, charger le fichier [books.json](../Data/books.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYet4x_YlcEn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkKMV_E9lcEq"
   },
   "source": [
    "Ecrire la fonction `mapper` nécessaire à la construction de l'index inversé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wc-TZ3OklcEr"
   },
   "outputs": [],
   "source": [
    "def mapper(data):\n",
    "    # A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irozkNzrlcEw"
   },
   "source": [
    "Ecrire la fonction `reducer` nécessaire à la construction de l'index inversé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSsoq9SflcEx"
   },
   "outputs": [],
   "source": [
    "def reducer(data): \n",
    "    # A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A88IjY5qlcEz"
   },
   "source": [
    "Tester vos deux fonctions en appliquant le code ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqcKWR75lcE0"
   },
   "outputs": [],
   "source": [
    "def invertedIndexExample(filename):\n",
    "    m = MapReduce(mapper, reducer)\n",
    "    results = m(readData(filename))\n",
    "    for w, b in results:\n",
    "        print(\"mot : \", w, \"livres : \", b)\n",
    "    return\n",
    "    \n",
    "invertedIndexExample('./Data/books.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bs8ftnbllcE2"
   },
   "source": [
    "## PARTIE 2 : Algorithme de PageRank\n",
    "\n",
    "Dans cette partie nous allons implémenter et étudier l'algorithme d'ordonnancement PageRank. Ce dernier est utilisé par le moteur de recherche Google pour attribuer un score d'importance à chaque page Web. D'ailleurs, c'est en partie sur la base de ce score (et de nombreux autres facteurs) que le moteur de recherche classe par ordre d'importance les pages Web correspondant à une recherche donnée.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1gL0RatlcE4"
   },
   "source": [
    "### Le format des données\n",
    "\n",
    "Nous allons utiliser le format de type \"graphe\" pour représenter un nombre $N$ de page Web. Voici un exemple de graphe :\n",
    "\n",
    "<img src=\"./Figures/graph.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "La première ligne indique le nombre de pages Web dont il est question pour le graphe. Puis pour chacune des lignes suivantes le premier numéro définit l'identifiant d'une page et les numéros suivants les liens sortants de cette page. \n",
    "\n",
    "Vous disposez dans le dossier [Data](./Data) associé à ce Lab de plusieurs graphes de tailles différentes.\n",
    "\n",
    "Comme nous allons manipuler des matrices dans cette partie, nous utiliserons la bibliothèque numpy que vous pouvez installer avec la commande ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SElo29uIlcE5"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Nd7IAcplcE8"
   },
   "source": [
    "#### Première étape : Matrice d'Adjacence.\n",
    "\n",
    "Nous nommons $A$ matrice d'adjacence, la matrice de dimension $N \\times N$ telle que $a_{i,j}=1$ s'il existe un lien de la page $i$ vers la page $j$, et $0$ sinon.\n",
    "\n",
    "\n",
    "**1. Ecrire une fonction `build_adjacency_matrix(filename)` permettant de construire la matrice d'adjacence à partir d'un graphe dont le format est défini plus haut (et au format .txt).**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "with open('./Data/graph1.txt', 'r') as f:\n",
    "    N = f.readline()\n",
    "    line = f.readline().split()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency_matrix = np.zeros((2,2))\n",
    "adjacency_matrix\n",
    "adjacency_matrix[0,0] = 1\n",
    "adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_N0La_ClcE-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_adjacendy_matrix(filename):\n",
    "    \"\"\"Build the adjacency matrix from the graph file\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        N = int(f.readline())\n",
    "        adjacency_matrix = np.zeros((N,N))\n",
    "        for _ in range(N):\n",
    "            line = f.readline().split()\n",
    "            depart = int(line.pop(0))\n",
    "            for column in line:\n",
    "                adjacency_matrix[depart,int(column)] = 1\n",
    "        return adjacency_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80tBTvC_lcFB"
   },
   "source": [
    "**2. Ouvrir le graph1.txt présent dans le dossier Data et dessiner le à la main avec des noeuds portant les numéros de pages et des flèches représentant les liens.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yXAAVtcplcFE"
   },
   "source": [
    "**3. Appliquer la fonction `build_adjacency_matrix()` au graph1.txt présent dans le dossier Data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQdhxacnlcFG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 1., 1., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_adjacendy_matrix('./Data/graph1.txt')# A completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XVxCBe6ylcFJ"
   },
   "source": [
    "#### Deuxième  étape : Matrice de transition.\n",
    "\n",
    "A partir de la matrice d'adjacence, il est possible de calculer une matrice de transition $P$ de paramètre $\\lambda$ dont nous aurons besoin lors de l'implémentation de l'algorithme de PageRank. La matrice de transition est définie par :\n",
    "\n",
    "$$ P_{i,j} =  \\left\\{\n",
    "\\begin{array}{l}\n",
    "   \\left.\\begin{array}{l} \n",
    "   \\lambda \\frac{a_{i,j}}{\\sum_{j=1}^N a_{i,j}} + (1-\\lambda ) \\frac{1}{N}  \\text{ si} \\sum_{j=1}^N a_{i,j} \\neq 0\\\\\n",
    "   \\frac{1}{N} \\text{ sinon.}\\\\\n",
    "   \\end{array}\\right.  \\\\\n",
    "\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "\n",
    "**4.  Ecrire une fonction `build_transition_matrix(filename, lam)` permettant de construire la matrice de transition à partir d'un graph (au format .txt) et d'une valeur pour le paramètre $\\lambda$ (correspondant à lam car lambda est un mot clé du langage python).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros((3,3))\n",
    "x\n",
    "len(x)\n",
    "list(range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35., 35., 35.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:] = 1\n",
    "x\n",
    "x[0,:] = x[0,:]/2\n",
    "x[0,:] = x[0,:]*10 + 30\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vZGjuI1lcFJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_transition_matrix(filename, lam):\n",
    "    \"\"\"Build the transition matrix from the adjacency matrix\"\"\"\n",
    "    # A completer\n",
    "    adjacency_matrix = build_adjacendy_matrix(filename)\n",
    "    N = len(adjacency_matrix)\n",
    "    for line in range(N):\n",
    "        somme = sum(adjacency_matrix[line,:])\n",
    "        if somme != 0:\n",
    "            adjacency_matrix[line,:] = (lam/somme)*adjacency_matrix[line,:] + (1-lam)/N\n",
    "        else:\n",
    "            adjacency_matrix[line,:] = 1/N\n",
    "        \n",
    "    return adjacency_matrix\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKu2Ux72lcFO"
   },
   "source": [
    "**5. Appliquer cette fonction au graph1.txt présent dans le dossier Data pour une valeur de lambda de 0.85 (valeur habituellement choisie).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p7uVT6lBlcFO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.025     , 0.025     , 0.45      , 0.45      , 0.025     ,\n",
       "        0.025     ],\n",
       "       [0.30833333, 0.025     , 0.025     , 0.30833333, 0.025     ,\n",
       "        0.30833333],\n",
       "       [0.025     , 0.2375    , 0.025     , 0.2375    , 0.2375    ,\n",
       "        0.2375    ],\n",
       "       [0.875     , 0.025     , 0.025     , 0.025     , 0.025     ,\n",
       "        0.025     ],\n",
       "       [0.025     , 0.025     , 0.025     , 0.875     , 0.025     ,\n",
       "        0.025     ],\n",
       "       [0.025     , 0.025     , 0.45      , 0.45      , 0.025     ,\n",
       "        0.025     ]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_transition_matrix(('./Data/graph1.txt'), 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpOdgiM3lcFR"
   },
   "source": [
    "**6. Appliquer la fonction somme sur les colonnes de la matrice de transition $P$ (sommes de tous les éléments d'une ligne, ligne par ligne). Que constatez-vous ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RxHVFWC_lcFT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A completer\n",
    "transition_matrix = build_transition_matrix(('./Data/graph1.txt'), 0.85)\n",
    "transition_matrix.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8QwfJxallcFW"
   },
   "source": [
    "**Interpretation de la matrice de transition**\n",
    "\n",
    "Le PageRank simule le chemin aléatoire d'un internaute qui naviguerait sur le Web en choisissant au hasard les liens à suivre sur une page donnée. Par exemple, dans le cas de la figure plus haut (en section intitulée \"le format des données\"), un internaute situé en page 1 choisirait d'aller à la page 0, 3 ou 2 avec probabilité 1/3, 1/3, 1/3 car il y a trois liens sortants de la page 1. Les éléments de la matrice de transition représentent en effet des probabilités de transition à chaque nœud qui peuvent être décrites globalement par la matrice de transition $P$ de dimension $N\\times N$, où $N$ est le nombre de nœuds dans le graphique et l'élément $p_{𝑖, 𝑗}$ est la probabilité qu'un internaute aléatoire passe de la page $i$ à la page $j$.\n",
    "\n",
    "\n",
    "**7. Dans le cas du graph1.txt, selon votre lecture visuelle du graph1 quelles sont les probabilités d'un internaute situé en page 5 d'aller dans chacunes des autres pages web ? Et selon votre matrice de transition, quelles sont ces mêmes probabilités ? Que constatez vous ?**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4sorE-3lcFW"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "# He can go to either page 2 or 3 with probability 1/2. In the transition matrix we take into account the probability\n",
    "# that the user \"jumps\" to any other page with a small probability 0,025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "swRLG-F4lcFb"
   },
   "source": [
    "**8. Calculer à nouveau la matrice de transition avec $\\lambda=1$. Que constatez-vous ? Nous reviendrons sur l'étude du paramètre $\\lambda$ et de son rôle par la suite.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HaAR-J6ElcFb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.5       , 0.5       , 0.        ,\n",
       "        0.        ],\n",
       "       [0.33333333, 0.        , 0.        , 0.33333333, 0.        ,\n",
       "        0.33333333],\n",
       "       [0.        , 0.25      , 0.        , 0.25      , 0.25      ,\n",
       "        0.25      ],\n",
       "       [1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.5       , 0.5       , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix = build_transition_matrix(('./Data/graph1.txt'), 1)\n",
    "transition_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8V6yKdflcFd"
   },
   "source": [
    "En parcourant plusieurs fois des balades aléatoires sur le graphe, PageRank simule le comportement de plusieurs internautes aléatoires. Les pages qui reçoivent un plus grand nombre de visites sont considérées comme plus importantes que celles qui ne le sont que rarement. \n",
    "\n",
    "\n",
    "Notons qu'au départ, quand un internaute commence sa promenade, il peut être n'importe où dans le graphe. Si nous n'avons aucune raison de penser qu'il serait plus susceptible de choisir une page plus qu'une autre comme point de départ, nous pouvons dire que la probabilité initiale que l'internaute se trouve sur une certaine page est égale à $\\frac{1}{N}$.\n",
    "\n",
    "Ainsi, au départ, la distribution de probabilité de la position de l'internaute peut être décrite par un vecteur colonne $R^{(0)}$ avec $N$ éléments prenant pour valeur $\\frac{1}{N}$. Ce vecteur s'appelle le vecteur PageRank à l'itération initiale $l=0$. Pour une itération quelconque nous le noterons $l$.\n",
    "\n",
    "\n",
    "#### Troisième  étape : Algorithme de PageRank\n",
    "\n",
    "Nous avons à ce moment en notre possession l'ensemble des éléments et outils pour implémenter facilement l'algorithme de PageRank suivant :\n",
    "\n",
    "<img src=\"./Figures/Algo_PR.png\" width=\"700\" height=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iqzzKfklcFe"
   },
   "source": [
    "**9. Implémenter l'Algorithme 1 par une fonction `page_rank(transition_matrix, epsilon = 1e-3):` en réutilisant toutes les fonctions que vous venez de construire plus haut et en fixant tout d'abord $\\lambda=0.85$. Attention : dans la partie initialisation, pensez à initialiser l'erreur $\\epsilon$, par exemple à la valeur $1$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16666667],\n",
       "       [0.16666667],\n",
       "       [0.16666667],\n",
       "       [0.16666667],\n",
       "       [0.16666667],\n",
       "       [0.16666667]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([3,4])\n",
    "x \n",
    "np.linalg.norm(x)\n",
    "N = 6\n",
    "R_1 = np.full((N, 1), 1/N)\n",
    "R_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QH1rg0bilcFf"
   },
   "outputs": [],
   "source": [
    "def page_rank(transition_matrix, epsilon=1):\n",
    "    \"\"\"Run the PageRank algorithm based on the transition matrix\"\"\"\n",
    "    N = len(transition_matrix)\n",
    "    R_1 = np.full((1, N), 1/N)\n",
    "    R_2 = np.dot(R_1, transition_matrix)\n",
    "    l = 0\n",
    "    while np.linalg.norm(R_2 - R_1) > epsilon:\n",
    "        l += 1\n",
    "        R_1 , R_2 = R_2, np.dot(R_2, transition_matrix)\n",
    "    #print(np.sum(R_1))\n",
    "    print('number of iterations for epsilon = {} : {}'.format(epsilon, l))\n",
    "    return R_1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fuD3Up4rlcFh"
   },
   "source": [
    "**10. Appliquez l'algorithme 1 au graph1.txt avec $\\lambda=0.85$ et $\\epsilon = 10^{-3}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tJqzm-VlcFh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations for epsilon = 0.0001 : 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.29902267, 0.06483634, 0.18740393, 0.30069957, 0.06483634,\n",
       "        0.08320116]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_rank(build_transition_matrix(('./Data/graph1.txt'), 0.85), 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcenhQKXlcFk"
   },
   "source": [
    "#### Quatrième étape : Etude et analyse de l'algorithme de PageRank implémenté.\n",
    "\n",
    "Rappel : fixez tout d'abord $\\lambda=0.85$.\n",
    "\n",
    "**11. Choisissez différentes valeurs pour le critère d'arrêt telles que $\\epsilon = {10^{-3}, 10^{-4}, 10^{-5}}$. Qu'est-ce que vous observez ?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "db8Lx6t8lcFl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For e = 10-3: [[0.29935795 0.06491206 0.18716453 0.30040839 0.06491206 0.08324501]]\n",
      "For e = 10-4: [[0.29902267 0.06483634 0.18740393 0.30069957 0.06483634 0.08320116]]\n",
      "For e = 10-5: [[0.29899138 0.06482929 0.18742625 0.3007267  0.06482929 0.08319708]]\n"
     ]
    }
   ],
   "source": [
    "# A compléter\n",
    "print('For e = 10-3:', page_rank(build_transition_matrix(('./Data/graph1.txt'), 0.85), 0.001))\n",
    "print('For e = 10-4:', page_rank(build_transition_matrix(('./Data/graph1.txt'), 0.85), 0.0001))\n",
    "print('For e = 10-5:',  page_rank(build_transition_matrix(('./Data/graph1.txt'), 0.85), 0.00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YIefxWjOlcFn"
   },
   "source": [
    "**12. Ajouter quelques hubs (pages qui ont beaucoup de liens sortant) et autorités (pages qui ont beaucoup de liens entrant). Quelles pages sont classées le plus haut ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-h_62AylcFn"
   },
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9mlHHa0lcFp"
   },
   "source": [
    "**13. Essayez d’accroître les rangs de certaines pages. Expliquez votre méthode et validez-la expérimentalement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DuV0OQPlcFq"
   },
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p98pkvOilcFs"
   },
   "source": [
    "**14. Essayez différentes valeurs pour le facteur d’amortissement $\\lambda$. Quel est le comportement de l’algorithme lorsque $\\lambda$ tend vers 0 ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9o2fZGERlcFs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIlwE5A4lcFu"
   },
   "source": [
    "**15. (À la maison) Amusez vous à tester l'algorithme sur les autres graphes plus volumineux mis à votre disposition dans le dossier Data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OdlLVeC3lcFu"
   },
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqqdYWDhlcFw"
   },
   "source": [
    "## PARTIE 3 : PageRank en mode distribué\n",
    "\n",
    "Nous allons nous intéresser ici au passage à l'échelle du page rank pour permettre de traiter les cas des graphes très volumineux comme le web.\n",
    "\n",
    "Vous devez avoir bien compris le principe de cet algoritme maintenant et votre premier travail sera donc d'appliquer le cadre MapReduce au calcul du PageRank. Dans un premier temps, vous pourrez ne pas prendre en compte le mécanisme de télétransportation pour vous faciliter la tâche.\n",
    "\n",
    "**1. Un exemple de graphe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COrb1KHHlcFx"
   },
   "source": [
    "Pour vous aider, vous pouvez vous aider d'un exemple de graphe simple comme celui ci-dessous. \n",
    "\n",
    "<img src=\"./Figures/graphes_lin.png\" width=\"700\" height=\"700\" />\n",
    "\n",
    "Source : Lin - Data-Intensive Text Processing with MapReduce\n",
    "\n",
    "\n",
    "A partir de cet exemple, pour vous aider à écrire votre fonction `map`, essayer de réflechir à comment le calcul du page rank transforme les différents noeuds à chaque itération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9PX3Y1blcFy"
   },
   "source": [
    "**2. Proposez en pseudo code un algorithme Mapeduce pour le calcul du PageRank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cndOw1kElcFy"
   },
   "outputs": [],
   "source": [
    "# A completer"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_4_MapR_PR_Student.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
